{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.en import English\n",
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "def batch_until(lines, condition=''):\n",
    "    # batch paragraphs\n",
    "    batch = ''\n",
    "    for l in lines:\n",
    "        _l = l.strip()\n",
    "        if _l == condition and batch != '':\n",
    "            yield batch\n",
    "            batch = ''\n",
    "        if _l == condition:\n",
    "            continue\n",
    "        batch += ' ' + _l\n",
    "\n",
    "\n",
    "def sentence_gen(lines):\n",
    "    # yield sentences\n",
    "    for doc in parser.pipe(batch_until(lines), batch_size=10, n_threads=2):\n",
    "        for s in doc.sents:\n",
    "            # sentinel values for begin/end sentence\n",
    "            yield '%s %s %s' % ('xsb xsb', s.text.strip(), 'xse')\n",
    "            \n",
    "            \n",
    "def tst_idx(tst_share=.05):\n",
    "    # indexes for test data\n",
    "    return np.random.choice(100, int(100*tst_share), replace=False)\n",
    "    \n",
    "    \n",
    "def trn_tst_split_sentence_gen(filename, tst_idx=None):\n",
    "    # yeild train/test data\n",
    "    with open(filename, 'r') as lines:\n",
    "        for idx, s in enumerate(sentence_gen(lines)):\n",
    "            if tst_idx is not None and (idx%100) in tst_idx:\n",
    "                yield s\n",
    "            elif tst_idx is not None:\n",
    "                continue\n",
    "            else:\n",
    "                yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_trial = './data/the_trial.txt'\n",
    "\n",
    "# sentence recognition can be non-trivial, so use spaCy\n",
    "parser = English()\n",
    "\n",
    "trn_sentence_gen = trn_tst_split_sentence_gen(the_trial)\n",
    "\n",
    "tstidx = tst_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INITIALIZE WORD COUNT MATRIX\n",
    "window_size = 3\n",
    "ngram = CountVectorizer(strip_accents='ascii', ngram_range=(1, window_size), \n",
    "                        lowercase=True, token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "ngram_dtm = ngram.fit_transform(trn_sentence_gen)\n",
    "\n",
    "ngram_vocab = np.array(ngram.get_feature_names())\n",
    "ngram_word_count = np.array(ngram_dtm.sum(axis=0))[0]\n",
    "\n",
    "words = [w for w in ngram.vocabulary_.keys() if ' ' not in w]\n",
    "\n",
    "vocab_size = len(words)\n",
    "\n",
    "_bp = ngram.build_preprocessor()\n",
    "_bt = ngram.build_tokenizer()\n",
    "# data transformer for test data\n",
    "sentence_transformer = lambda s: _bt(_bp(ngram.decode(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_count = lambda w: vocab_size if w=='' else 0.0 if ngram.vocabulary_.get(w) is None else ngram_word_count[ngram.vocabulary_.get(w)] \n",
    "\n",
    "def ngram_prob(ngram_str, n_words_doc):\n",
    "    if ngram_str == '':\n",
    "        # 0-gram\n",
    "        return 1/n_words_doc\n",
    "    conditioning = ngram_str.split()[:-1]\n",
    "    numer = ngram_count(ngram_str)\n",
    "    if len(conditioning) == 0:\n",
    "        # uni-gram\n",
    "        return numer/n_words_doc\n",
    "    conditioning = ngram_count(' '.join(conditioning))\n",
    "    if conditioning == 0.0:\n",
    "        # out of vocabulary\n",
    "        return conditioning\n",
    "    # n-gram\n",
    "    return ngram_count(ngram_str)/conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.78524401e-01   1.87363737e-01   3.39541878e-02   1.57673601e-04]\n",
      "[  9.31495186e-01   6.60160085e-02   2.48875833e-03   4.69064058e-08]\n",
      "[  9.78093474e-01   2.17345508e-02   1.71974955e-04   1.27148778e-11]\n",
      "[  9.92925219e-01   7.06308529e-03   1.16954631e-05   3.36252720e-15]\n",
      "[  9.97711739e-01   2.28746918e-03   7.91531641e-07   8.82578036e-19]\n"
     ]
    }
   ],
   "source": [
    "# LEARN LAMBDA VALUES FROM EM ALGORITHM\n",
    "\n",
    "n_words_doc = functools.reduce(lambda a, b: a+b, [ngram_word_count[ngram.vocabulary_.get(w)] for w in words])\n",
    "\n",
    "# [('xsb xsb now', 'xsb now', 'now', ''),\n",
    "#  ('xsb now they', 'now they', 'they', ''), ...]\n",
    "to_ngrams = lambda s: [tuple(' '.join(s[(idx-3+_idx):idx]) for _idx in range(4)) for idx in range(3, len(s)+1)]\n",
    "\n",
    "# [[0.010, 0.005, 0.065, 0.000], ...]\n",
    "to_ngram_vals = lambda ngrams: [[ngram_prob(ng, n_words_doc) for ng in ngs] for ngs in ngrams]\n",
    "\n",
    "# ls: initial lambda values\n",
    "ls = np.array([.25,.25,.25,.25])\n",
    "\n",
    "# 5 iterations over EM\n",
    "for _ in range(5):\n",
    "    # ngram values\n",
    "    ng_arr = np.zeros(4)\n",
    "    for str_hld in trn_tst_split_sentence_gen(the_trial, tstidx):\n",
    "        transformed = sentence_transformer(str_hld)\n",
    "        ngrams = to_ngrams(transformed)        \n",
    "        ngvals = to_ngram_vals(ngrams)\n",
    "        # Expectation\n",
    "        expctations = [ls*ngval for ngval in ngvals]\n",
    "        for e in expctations:\n",
    "            # z normalizing constant\n",
    "            z = e.sum()\n",
    "            ng_arr += e/z\n",
    "\n",
    "    # Maximization        \n",
    "    ls = ng_arr/ng_arr.sum()\n",
    "    print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why did the job i ve attacked you added k .\n",
      "\n",
      "the businessman and anyway i want to miss montag made a sign from her fault or not .\n",
      "\n",
      "block listened closely with his hand and then in the country had not seen the two of them if i m concerned i ve heard about your case there was nowhere to be dismissed from his work when he had been unable to ask her advice .\n",
      "\n",
      "there were however also to have a few bland phrases which the court offices .\n",
      "\n",
      "all of them had gathered together down in the waiting room here it would never have arisen in the pockets of his collar .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GERNATE NEW TEXT\n",
    "\n",
    "def gen_word(context):\n",
    "    r = np.random.random()\n",
    "    acc_sum = 0.0\n",
    "    for w in words:\n",
    "        snt = '%s %s' % (context, w)\n",
    "        transformed = sentence_transformer(snt)\n",
    "        ngrams = to_ngrams(transformed)        \n",
    "        ngvals = to_ngram_vals(ngrams)[0]\n",
    "        acc_sum += (ls*ngvals).sum()\n",
    "        if acc_sum>=r:\n",
    "            if w == 'xsb':\n",
    "                continue\n",
    "            if w == 'xse':\n",
    "                return '.'\n",
    "            return w\n",
    "\n",
    "def gen_sentence():\n",
    "    context = ['xsb', 'xsb']\n",
    "    sent = []\n",
    "    while True:\n",
    "        w = gen_word(' '.join(context))\n",
    "        if w == context[1]:\n",
    "            continue\n",
    "        sent.append(w)\n",
    "        context[0] = context[1]\n",
    "        context[1] = w\n",
    "        if w == '.':\n",
    "            return ' '.join(sent)\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    print(gen_sentence() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4592283199900509"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERPLEXITY MODEL EVALUATION \n",
    "\n",
    "tst_n = 0 # length normalization\n",
    "accum_prob = 0\n",
    "for str_hld in trn_tst_split_sentence_gen(the_trial, tstidx):\n",
    "    tstrsplt=sentence_transformer(str_hld)\n",
    "    ngrams=[tuple(' '.join(tstrsplt[(idx-3+_idx):idx]) for _idx in range(4)) for idx in range(3, len(tstrsplt)+1)]\n",
    "    tst_n += len(ngrams)\n",
    "    ngvals=[[ngram_prob(ng, n_words_doc) for ng in ngs] for ngs in ngrams]\n",
    "    x = [ls*ngval for ngval in ngvals]\n",
    "    for _x in x:\n",
    "        accum_prob -= np.log(_x.sum())\n",
    "       \n",
    "np.exp(accum_prob/tst_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
